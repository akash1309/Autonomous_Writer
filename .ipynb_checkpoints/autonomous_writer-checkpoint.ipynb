{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAs8LxFeB72p"
   },
   "source": [
    "#Predicting a sequence of words using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wv1uvScN-et_"
   },
   "source": [
    "## 1) Importing the basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdN5S4phBpdN"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-12581fbc1ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PfbmjOm-nbn"
   },
   "source": [
    "## 2) Reading the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5HarfUZOCW78",
    "outputId": "5337f3bb-dc6b-48f2-9aa0-4a2b8cef24bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609, str)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/shakespeare.txt') as f:\n",
    "  text = f.read()\n",
    "\n",
    "len(text), type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DQ-M-GZMDSme",
    "outputId": "7de8e09e-2c61-4760-9f57-82c15629be78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "RH7ATppeIsMu",
    "outputId": "b8935b0b-543d-4f12-c332-c5f3d61c632d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7lhn_R0JEe3"
   },
   "source": [
    "## 3) Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "s8bmKO3FI4tw",
    "outputId": "e8f282cc-65a2-46de-c9c5-44a214a0a1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 \n",
      "\n",
      "{'t', 'N', 'D', 'L', '7', '}', '\\n', 'a', '>', '2', 'J', ']', 'B', '|', 'U', 'b', 'h', 'o', 'Z', 'P', 'C', '9', 'm', \"'\", 'I', 'V', 'T', 'O', 'v', 'y', 'r', 'k', 'l', 'H', 'A', 'W', 'F', '.', '4', '&', 'G', 'z', ':', 'p', 'M', '0', '[', 'f', 'j', 'X', ' ', 'w', 'g', '5', 'K', 'x', 'Q', 'Y', '(', 'c', 'u', '1', ';', 'E', 'R', '<', '!', '_', '6', ',', '3', 'd', '8', '?', 's', '\"', '`', 'i', ')', 'e', 'S', 'q', '-', 'n'}\n"
     ]
    }
   ],
   "source": [
    "# We are going for character level encoding\n",
    "\n",
    "character_set = set(text)\n",
    "print(f'{len(character_set)} \\n')\n",
    "print(np.array(character_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufVa508yJUb6"
   },
   "outputs": [],
   "source": [
    "#Creating encoder and decoder\n",
    "\n",
    "#Decoder (num --> letter)\n",
    "\n",
    "decoder = dict(enumerate(character_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9xCpDdOKDkd"
   },
   "outputs": [],
   "source": [
    "#Encoder (letter --> num)\n",
    "\n",
    "encoder = {each:idx for idx,each in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "AfbVw_wgKme2",
    "outputId": "1294ab69-9243-4758-e5eb-aa001e33fa1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "       50, 50, 50, 50, 50, 61,  6, 50, 50, 36, 30, 17, 22, 50, 47,  7, 77,\n",
       "       30, 79, 74,  0, 50, 59, 30, 79,  7,  0, 60, 30, 79, 74, 50, 51, 79,\n",
       "       50, 71, 79, 74, 77, 30, 79, 50, 77, 83, 59, 30, 79,  7, 74, 79, 69,\n",
       "        6, 50, 50, 26, 16,  7,  0, 50,  0, 16, 79, 30, 79, 15, 29, 50, 15,\n",
       "       79,  7, 60,  0, 29, 23, 74, 50, 30, 17, 74, 79, 50, 22, 77, 52, 16,\n",
       "        0, 50, 83, 79, 28, 79, 30, 50, 71, 77, 79, 69,  6, 50, 50, 12, 60,\n",
       "        0, 50,  7, 74, 50,  0, 16, 79, 50, 30, 77, 43, 79, 30, 50, 74, 16,\n",
       "       17, 60, 32, 71, 50, 15, 29, 50,  0, 77, 22, 79, 50, 71, 79, 59, 79,\n",
       "        7, 74, 79, 69,  6, 50, 50, 33, 77, 74, 50,  0, 79, 83, 71, 79, 30,\n",
       "       50, 16, 79, 77, 30, 50, 22, 77, 52, 16,  0, 50, 15, 79,  7, 30, 50,\n",
       "       16, 77, 74, 50, 22, 79, 22, 17, 30, 29, 42,  6, 50, 50, 12, 60,  0,\n",
       "       50,  0, 16, 17, 60, 50, 59, 17, 83,  0, 30,  7, 59,  0, 79, 71, 50,\n",
       "        0, 17, 50,  0, 16, 77, 83, 79, 50, 17, 51, 83, 50, 15, 30, 77, 52,\n",
       "       16,  0, 50, 79, 29, 79, 74, 69,  6, 50, 50, 36, 79, 79, 71, 23, 74,\n",
       "        0, 50,  0, 16, 29, 50, 32, 77, 52, 16,  0, 23, 74, 50, 47, 32,  7,\n",
       "       22, 79, 50, 51, 77,  0, 16, 50, 74, 79, 32, 47, 82, 74, 60, 15, 74,\n",
       "        0,  7, 83,  0, 77,  7, 32, 50, 47, 60, 79, 32, 69,  6, 50, 50, 44,\n",
       "        7, 31, 77, 83, 52, 50,  7, 50, 47,  7, 22, 77, 83, 79, 50, 51, 16,\n",
       "       79, 30, 79, 50,  7, 15, 60, 83, 71,  7, 83, 59, 79, 50, 32, 77, 79,\n",
       "       74, 69,  6, 50, 50, 26, 16, 29, 50, 74, 79, 32, 47, 50,  0, 16, 29,\n",
       "       50, 47, 17, 79, 69, 50,  0, 17, 50,  0, 16, 29, 50, 74, 51, 79, 79,\n",
       "        0, 50, 74, 79, 32, 47, 50,  0, 17, 17, 50, 59, 30, 60, 79, 32, 42,\n",
       "        6, 50, 50, 26, 16, 17, 60, 50,  0, 16,  7,  0, 50,  7, 30,  0, 50,\n",
       "       83, 17, 51, 50,  0, 16, 79, 50, 51, 17, 30, 32, 71, 23, 74, 50, 47,\n",
       "       30, 79, 74, 16, 50, 17, 30, 83,  7, 22, 79, 83,  0, 69,  6, 50, 50,\n",
       "       34, 83, 71, 50, 17, 83, 32, 29, 50, 16, 79, 30,  7, 32, 71, 50,  0,\n",
       "       17, 50,  0, 16, 79, 50, 52,  7, 60, 71, 29, 50, 74, 43, 30, 77, 83,\n",
       "       52, 69,  6, 50, 50, 35, 77,  0, 16, 77, 83, 50,  0, 16, 77, 83, 79,\n",
       "       50, 17, 51, 83, 50, 15, 60])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the text into numerical values\n",
    "\n",
    "encoded_text = np.array([encoder[ch] for ch in text])\n",
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OHpDC8_E8zFP",
    "outputId": "37d3fbd6-5736-414f-fc88-43aef9fade75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 50, 50, ..., 63,  1,  2])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQOlDxSs9jqX"
   },
   "source": [
    "To convert a numpy array into one hot encoding : https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzdQlnQX-wao"
   },
   "source": [
    "### 3.1) Creating one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeCafhB63Ox4"
   },
   "outputs": [],
   "source": [
    "#Creating one-hot encoder for the encoded_text\n",
    "\n",
    "def one_hot_encoder(encoded_text,num_uni_chars):\n",
    "  #encoded text ---> Batch of encoded text\n",
    "  #num_uni_chars ---> len(set(text))\n",
    "\n",
    "  one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
    "\n",
    "  one_hot = one_hot.astype(np.float32)  #for tensor\n",
    "\n",
    "  one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
    "\n",
    "  one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
    "\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "k4QEJRaT58T-",
    "outputId": "d6c72630-0bca-4b3f-c01c-1ecb920cc261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,0])\n",
    "print(a)\n",
    "one_hot_encoder(a,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glL2X_kt-1Va"
   },
   "source": [
    "## 4) Generating Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtehZVh5-JPR"
   },
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text,samp_per_batch=10,seq_length=50):\n",
    "  \n",
    "  # X  = encoded text of len sequence length(x to x+50)  \n",
    "  # y = encoded text of len sequence length(x+1 to x+51)\n",
    "  # For example : if encoded_text has [0,1,2,3,4,5,6] and seq_len = 3, then X = [0,1,2] , y = [1,2,3]\n",
    "\n",
    "  #How many characters per batch ??\n",
    "  char_per_batch = samp_per_batch * seq_length\n",
    "\n",
    "  # How many batches we can make, given the length of encoding text ??\n",
    "  num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "\n",
    "  #Cut off the borders of the encoded text that does not fit in this range\n",
    "  encoded_text = encoded_text[:char_per_batch * num_batches_avail]\n",
    "\n",
    "  encoded_text = encoded_text.reshape((samp_per_batch,-1))\n",
    "\n",
    "  # Go through each row in array.\n",
    "  for n in range(0, encoded_text.shape[1], seq_length):\n",
    "      \n",
    "    # Grab feature characters\n",
    "    x = encoded_text[:, n:n+seq_length]\n",
    "    \n",
    "    # y is the target shifted over by 1\n",
    "    y = np.zeros_like(x)\n",
    "    \n",
    "    #\n",
    "    try:\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        y[:, -1]  = encoded_text[:, n+seq_length]\n",
    "        \n",
    "    # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "    except:\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        y[:, -1] = encoded_text[:, 0]\n",
    "        \n",
    "    yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MY4ZCWkjaz8Z",
    "outputId": "05df3aaf-662e-41c1-ec0e-676da506cb59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = np.arange(20)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EVlD7_R8a8uu",
    "outputId": "a7fd6155-5427-4b83-b8eb-1e1b2e6302a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n",
      "[[ 1  2  3  4  5  6  7  8  9  0]\n",
      " [11 12 13 14 15 16 17 18 19 10]]\n"
     ]
    }
   ],
   "source": [
    "sam = generate_batches(sample_text,2,10)\n",
    "x,y = next(sam)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYVJR9RNdF1K"
   },
   "source": [
    "## 5) Creating the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkrkrbRcbd2n"
   },
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "  def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "      \n",
    "      \n",
    "      # SET UP ATTRIBUTES\n",
    "      super().__init__()\n",
    "      self.drop_prob = drop_prob\n",
    "      self.num_layers = num_layers\n",
    "      self.num_hidden = num_hidden\n",
    "      self.use_gpu = use_gpu\n",
    "      \n",
    "      #CHARACTER SET, ENCODER, and DECODER\n",
    "      self.all_chars = all_chars\n",
    "      self.decoder = dict(enumerate(all_chars))\n",
    "      self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "      \n",
    "      \n",
    "      self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "      \n",
    "      self.dropout = nn.Dropout(drop_prob)\n",
    "      \n",
    "      self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "    \n",
    "  \n",
    "  def forward(self, x, hidden):\n",
    "                \n",
    "      \n",
    "      lstm_output, hidden = self.lstm(x, hidden)\n",
    "      \n",
    "      \n",
    "      drop_output = self.dropout(lstm_output)\n",
    "      \n",
    "      drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "      \n",
    "      \n",
    "      final_out = self.fc_linear(drop_output)\n",
    "      \n",
    "      \n",
    "      return final_out, hidden\n",
    "  \n",
    "  \n",
    "  def hidden_state(self, batch_size):\n",
    "      \n",
    "      if self.use_gpu:\n",
    "          \n",
    "          hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                    torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "      else:\n",
    "          hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                    torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "      \n",
    "      return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr9lDn4VicCG"
   },
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=character_set,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkFmfbrlilxy"
   },
   "outputs": [],
   "source": [
    "total_param  = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hJ3bGemBjGlm",
    "outputId": "9f0d510d-56b4-4b00-9422-7d06c4fdad0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu1rGAJojRGo"
   },
   "source": [
    "## 6) Optimization and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JTsPBScjKJW"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9r61KwUkK_I"
   },
   "source": [
    "## 7) Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VuM9iZMcjWA-",
    "outputId": "1583e9e4-b4aa-42f3-f432-9b7519d24998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_percent = 0.9\n",
    "int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZFiE-_JkbRM"
   },
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Cpz3wbwk4mu"
   },
   "source": [
    "## 8) Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRTjTVMqk1P2"
   },
   "outputs": [],
   "source": [
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 50\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "08m-fuxllIdo",
    "outputId": "80ca07e7-268e-464b-a033-7e4549a4c0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.215010404586792\n",
      "Epoch: 0 Step: 50 Val Loss: 3.2082674503326416\n",
      "Epoch: 0 Step: 75 Val Loss: 6.7060418128967285\n",
      "Epoch: 0 Step: 100 Val Loss: 3.2046127319335938\n",
      "Epoch: 0 Step: 125 Val Loss: 3.146069288253784\n",
      "Epoch: 0 Step: 150 Val Loss: 2.9709537029266357\n",
      "Epoch: 0 Step: 175 Val Loss: 2.8704428672790527\n",
      "Epoch: 0 Step: 200 Val Loss: 2.719435930252075\n",
      "Epoch: 0 Step: 225 Val Loss: 2.627060651779175\n",
      "Epoch: 0 Step: 250 Val Loss: 2.5036046504974365\n",
      "Epoch: 0 Step: 275 Val Loss: 2.3791403770446777\n",
      "Epoch: 0 Step: 300 Val Loss: 2.276585102081299\n",
      "Epoch: 0 Step: 325 Val Loss: 2.2038164138793945\n",
      "Epoch: 0 Step: 350 Val Loss: 2.1464829444885254\n",
      "Epoch: 0 Step: 375 Val Loss: 2.099104642868042\n",
      "Epoch: 1 Step: 400 Val Loss: 2.05954647064209\n",
      "Epoch: 1 Step: 425 Val Loss: 2.025606393814087\n",
      "Epoch: 1 Step: 450 Val Loss: 1.985239863395691\n",
      "Epoch: 1 Step: 475 Val Loss: 1.951309323310852\n",
      "Epoch: 1 Step: 500 Val Loss: 1.9205573797225952\n",
      "Epoch: 1 Step: 525 Val Loss: 1.894120216369629\n",
      "Epoch: 1 Step: 550 Val Loss: 1.868466854095459\n",
      "Epoch: 1 Step: 575 Val Loss: 1.840869426727295\n",
      "Epoch: 1 Step: 600 Val Loss: 1.8201916217803955\n",
      "Epoch: 1 Step: 625 Val Loss: 1.8004237413406372\n",
      "Epoch: 1 Step: 650 Val Loss: 1.7834696769714355\n",
      "Epoch: 1 Step: 675 Val Loss: 1.7597087621688843\n",
      "Epoch: 1 Step: 700 Val Loss: 1.7423574924468994\n",
      "Epoch: 1 Step: 725 Val Loss: 1.730948805809021\n",
      "Epoch: 1 Step: 750 Val Loss: 1.708878993988037\n",
      "Epoch: 2 Step: 775 Val Loss: 1.6993991136550903\n",
      "Epoch: 2 Step: 800 Val Loss: 1.6810413599014282\n",
      "Epoch: 2 Step: 825 Val Loss: 1.6648225784301758\n",
      "Epoch: 2 Step: 850 Val Loss: 1.6545944213867188\n",
      "Epoch: 2 Step: 875 Val Loss: 1.6391600370407104\n",
      "Epoch: 2 Step: 900 Val Loss: 1.6270064115524292\n",
      "Epoch: 2 Step: 925 Val Loss: 1.614121437072754\n",
      "Epoch: 2 Step: 950 Val Loss: 1.6023718118667603\n",
      "Epoch: 2 Step: 975 Val Loss: 1.5936702489852905\n",
      "Epoch: 2 Step: 1000 Val Loss: 1.5857092142105103\n",
      "Epoch: 2 Step: 1025 Val Loss: 1.5753523111343384\n",
      "Epoch: 2 Step: 1050 Val Loss: 1.5682483911514282\n",
      "Epoch: 2 Step: 1075 Val Loss: 1.5586175918579102\n",
      "Epoch: 2 Step: 1100 Val Loss: 1.5475218296051025\n",
      "Epoch: 2 Step: 1125 Val Loss: 1.5418740510940552\n",
      "Epoch: 3 Step: 1150 Val Loss: 1.5409252643585205\n",
      "Epoch: 3 Step: 1175 Val Loss: 1.532901406288147\n",
      "Epoch: 3 Step: 1200 Val Loss: 1.5223972797393799\n",
      "Epoch: 3 Step: 1225 Val Loss: 1.514207363128662\n",
      "Epoch: 3 Step: 1250 Val Loss: 1.5063608884811401\n",
      "Epoch: 3 Step: 1275 Val Loss: 1.4991978406906128\n",
      "Epoch: 3 Step: 1300 Val Loss: 1.5010654926300049\n",
      "Epoch: 3 Step: 1325 Val Loss: 1.4899744987487793\n",
      "Epoch: 3 Step: 1350 Val Loss: 1.4864089488983154\n",
      "Epoch: 3 Step: 1375 Val Loss: 1.4834187030792236\n",
      "Epoch: 3 Step: 1400 Val Loss: 1.4783117771148682\n",
      "Epoch: 3 Step: 1425 Val Loss: 1.4708845615386963\n",
      "Epoch: 3 Step: 1450 Val Loss: 1.461584448814392\n",
      "Epoch: 3 Step: 1475 Val Loss: 1.458608865737915\n",
      "Epoch: 3 Step: 1500 Val Loss: 1.4544494152069092\n",
      "Epoch: 3 Step: 1525 Val Loss: 1.4507331848144531\n",
      "Epoch: 4 Step: 1550 Val Loss: 1.448777198791504\n",
      "Epoch: 4 Step: 1575 Val Loss: 1.4420720338821411\n",
      "Epoch: 4 Step: 1600 Val Loss: 1.4389469623565674\n",
      "Epoch: 4 Step: 1625 Val Loss: 1.434136152267456\n",
      "Epoch: 4 Step: 1650 Val Loss: 1.429200291633606\n",
      "Epoch: 4 Step: 1675 Val Loss: 1.4311603307724\n",
      "Epoch: 4 Step: 1700 Val Loss: 1.4231271743774414\n",
      "Epoch: 4 Step: 1725 Val Loss: 1.4204504489898682\n",
      "Epoch: 4 Step: 1750 Val Loss: 1.4113647937774658\n",
      "Epoch: 4 Step: 1775 Val Loss: 1.4081250429153442\n",
      "Epoch: 4 Step: 1800 Val Loss: 1.4113365411758423\n",
      "Epoch: 4 Step: 1825 Val Loss: 1.4075369834899902\n",
      "Epoch: 4 Step: 1850 Val Loss: 1.4077039957046509\n",
      "Epoch: 4 Step: 1875 Val Loss: 1.40288507938385\n",
      "Epoch: 4 Step: 1900 Val Loss: 1.4004006385803223\n",
      "Epoch: 5 Step: 1925 Val Loss: 1.398428201675415\n",
      "Epoch: 5 Step: 1950 Val Loss: 1.393336296081543\n",
      "Epoch: 5 Step: 1975 Val Loss: 1.3913263082504272\n",
      "Epoch: 5 Step: 2000 Val Loss: 1.3844059705734253\n",
      "Epoch: 5 Step: 2025 Val Loss: 1.3878886699676514\n",
      "Epoch: 5 Step: 2050 Val Loss: 1.3883335590362549\n",
      "Epoch: 5 Step: 2075 Val Loss: 1.3830115795135498\n",
      "Epoch: 5 Step: 2100 Val Loss: 1.3747773170471191\n",
      "Epoch: 5 Step: 2125 Val Loss: 1.3741542100906372\n",
      "Epoch: 5 Step: 2150 Val Loss: 1.3712660074234009\n",
      "Epoch: 5 Step: 2175 Val Loss: 1.374729037284851\n",
      "Epoch: 5 Step: 2200 Val Loss: 1.3848702907562256\n",
      "Epoch: 5 Step: 2225 Val Loss: 1.3682118654251099\n",
      "Epoch: 5 Step: 2250 Val Loss: 1.364441990852356\n",
      "Epoch: 5 Step: 2275 Val Loss: 1.3654582500457764\n",
      "Epoch: 6 Step: 2300 Val Loss: 1.3634306192398071\n",
      "Epoch: 6 Step: 2325 Val Loss: 1.363244652748108\n",
      "Epoch: 6 Step: 2350 Val Loss: 1.3598365783691406\n",
      "Epoch: 6 Step: 2375 Val Loss: 1.355495810508728\n",
      "Epoch: 6 Step: 2400 Val Loss: 1.3595050573349\n",
      "Epoch: 6 Step: 2425 Val Loss: 1.3608031272888184\n",
      "Epoch: 6 Step: 2450 Val Loss: 1.3547675609588623\n",
      "Epoch: 6 Step: 2475 Val Loss: 1.3505911827087402\n",
      "Epoch: 6 Step: 2500 Val Loss: 1.3526523113250732\n",
      "Epoch: 6 Step: 2525 Val Loss: 1.3474465608596802\n",
      "Epoch: 6 Step: 2550 Val Loss: 1.349308729171753\n",
      "Epoch: 6 Step: 2575 Val Loss: 1.3517918586730957\n",
      "Epoch: 6 Step: 2600 Val Loss: 1.3444758653640747\n",
      "Epoch: 6 Step: 2625 Val Loss: 1.3442296981811523\n",
      "Epoch: 6 Step: 2650 Val Loss: 1.3467217683792114\n",
      "Epoch: 7 Step: 2675 Val Loss: 1.3459101915359497\n",
      "Epoch: 7 Step: 2700 Val Loss: 1.3434559106826782\n",
      "Epoch: 7 Step: 2725 Val Loss: 1.3410650491714478\n",
      "Epoch: 7 Step: 2750 Val Loss: 1.3388879299163818\n",
      "Epoch: 7 Step: 2775 Val Loss: 1.3359718322753906\n",
      "Epoch: 7 Step: 2800 Val Loss: 1.3384960889816284\n",
      "Epoch: 7 Step: 2825 Val Loss: 1.336802363395691\n",
      "Epoch: 7 Step: 2850 Val Loss: 1.3354132175445557\n",
      "Epoch: 7 Step: 2875 Val Loss: 1.3306143283843994\n",
      "Epoch: 7 Step: 2900 Val Loss: 1.328696608543396\n",
      "Epoch: 7 Step: 2925 Val Loss: 1.3344730138778687\n",
      "Epoch: 7 Step: 2950 Val Loss: 1.3319816589355469\n",
      "Epoch: 7 Step: 2975 Val Loss: 1.3306472301483154\n",
      "Epoch: 7 Step: 3000 Val Loss: 1.3263229131698608\n",
      "Epoch: 7 Step: 3025 Val Loss: 1.3334835767745972\n",
      "Epoch: 7 Step: 3050 Val Loss: 1.3261425495147705\n",
      "Epoch: 8 Step: 3075 Val Loss: 1.3305871486663818\n",
      "Epoch: 8 Step: 3100 Val Loss: 1.3252404928207397\n",
      "Epoch: 8 Step: 3125 Val Loss: 1.3263572454452515\n",
      "Epoch: 8 Step: 3150 Val Loss: 1.3221327066421509\n",
      "Epoch: 8 Step: 3175 Val Loss: 1.3256068229675293\n",
      "Epoch: 8 Step: 3200 Val Loss: 1.3204339742660522\n",
      "Epoch: 8 Step: 3225 Val Loss: 1.3221771717071533\n",
      "Epoch: 8 Step: 3250 Val Loss: 1.317990779876709\n",
      "Epoch: 8 Step: 3275 Val Loss: 1.3209145069122314\n",
      "Epoch: 8 Step: 3300 Val Loss: 1.3199496269226074\n",
      "Epoch: 8 Step: 3325 Val Loss: 1.3177859783172607\n",
      "Epoch: 8 Step: 3350 Val Loss: 1.318239688873291\n",
      "Epoch: 8 Step: 3375 Val Loss: 1.313165307044983\n",
      "Epoch: 8 Step: 3400 Val Loss: 1.3251374959945679\n",
      "Epoch: 8 Step: 3425 Val Loss: 1.3200325965881348\n",
      "Epoch: 9 Step: 3450 Val Loss: 1.3165112733840942\n",
      "Epoch: 9 Step: 3475 Val Loss: 1.3126096725463867\n",
      "Epoch: 9 Step: 3500 Val Loss: 1.3172944784164429\n",
      "Epoch: 9 Step: 3525 Val Loss: 1.315407156944275\n",
      "Epoch: 9 Step: 3550 Val Loss: 1.3202441930770874\n",
      "Epoch: 9 Step: 3575 Val Loss: 1.3122596740722656\n",
      "Epoch: 9 Step: 3600 Val Loss: 1.3090057373046875\n",
      "Epoch: 9 Step: 3625 Val Loss: 1.3119860887527466\n",
      "Epoch: 9 Step: 3650 Val Loss: 1.3087303638458252\n",
      "Epoch: 9 Step: 3675 Val Loss: 1.3109581470489502\n",
      "Epoch: 9 Step: 3700 Val Loss: 1.3070552349090576\n",
      "Epoch: 9 Step: 3725 Val Loss: 1.3085527420043945\n",
      "Epoch: 9 Step: 3750 Val Loss: 1.3048794269561768\n",
      "Epoch: 9 Step: 3775 Val Loss: 1.3053303956985474\n",
      "Epoch: 9 Step: 3800 Val Loss: 1.3047664165496826\n",
      "Epoch: 10 Step: 3825 Val Loss: 1.3069407939910889\n",
      "Epoch: 10 Step: 3850 Val Loss: 1.3064827919006348\n",
      "Epoch: 10 Step: 3875 Val Loss: 1.3062747716903687\n",
      "Epoch: 10 Step: 3900 Val Loss: 1.3036364316940308\n",
      "Epoch: 10 Step: 3925 Val Loss: 1.304503321647644\n",
      "Epoch: 10 Step: 3950 Val Loss: 1.298764944076538\n",
      "Epoch: 10 Step: 3975 Val Loss: 1.3002102375030518\n",
      "Epoch: 10 Step: 4000 Val Loss: 1.300809621810913\n",
      "Epoch: 10 Step: 4025 Val Loss: 1.3032459020614624\n",
      "Epoch: 10 Step: 4050 Val Loss: 1.3018302917480469\n",
      "Epoch: 10 Step: 4075 Val Loss: 1.3029372692108154\n",
      "Epoch: 10 Step: 4100 Val Loss: 1.2972697019577026\n",
      "Epoch: 10 Step: 4125 Val Loss: 1.2970472574234009\n",
      "Epoch: 10 Step: 4150 Val Loss: 1.298912763595581\n",
      "Epoch: 10 Step: 4175 Val Loss: 1.2970755100250244\n",
      "Epoch: 10 Step: 4200 Val Loss: 1.2981078624725342\n",
      "Epoch: 11 Step: 4225 Val Loss: 1.2972007989883423\n",
      "Epoch: 11 Step: 4250 Val Loss: 1.2970279455184937\n",
      "Epoch: 11 Step: 4275 Val Loss: 1.2956947088241577\n",
      "Epoch: 11 Step: 4300 Val Loss: 1.29266357421875\n",
      "Epoch: 11 Step: 4325 Val Loss: 1.2913035154342651\n",
      "Epoch: 11 Step: 4350 Val Loss: 1.2946985960006714\n",
      "Epoch: 11 Step: 4375 Val Loss: 1.2945741415023804\n",
      "Epoch: 11 Step: 4400 Val Loss: 1.2938786745071411\n",
      "Epoch: 11 Step: 4425 Val Loss: 1.2968772649765015\n",
      "Epoch: 11 Step: 4450 Val Loss: 1.2966108322143555\n",
      "Epoch: 11 Step: 4475 Val Loss: 1.291337013244629\n",
      "Epoch: 11 Step: 4500 Val Loss: 1.2953776121139526\n",
      "Epoch: 11 Step: 4525 Val Loss: 1.2938051223754883\n",
      "Epoch: 11 Step: 4550 Val Loss: 1.2938361167907715\n",
      "Epoch: 11 Step: 4575 Val Loss: 1.28934645652771\n",
      "Epoch: 12 Step: 4600 Val Loss: 1.2926437854766846\n",
      "Epoch: 12 Step: 4625 Val Loss: 1.2899131774902344\n",
      "Epoch: 12 Step: 4650 Val Loss: 1.2926138639450073\n",
      "Epoch: 12 Step: 4675 Val Loss: 1.2890396118164062\n",
      "Epoch: 12 Step: 4700 Val Loss: 1.2886009216308594\n",
      "Epoch: 12 Step: 4725 Val Loss: 1.2919915914535522\n",
      "Epoch: 12 Step: 4750 Val Loss: 1.291337251663208\n",
      "Epoch: 12 Step: 4775 Val Loss: 1.2874799966812134\n",
      "Epoch: 12 Step: 4800 Val Loss: 1.2862950563430786\n",
      "Epoch: 12 Step: 4825 Val Loss: 1.286406397819519\n",
      "Epoch: 12 Step: 4850 Val Loss: 1.2859607934951782\n",
      "Epoch: 12 Step: 4875 Val Loss: 1.288295030593872\n",
      "Epoch: 12 Step: 4900 Val Loss: 1.284927487373352\n",
      "Epoch: 12 Step: 4925 Val Loss: 1.284204125404358\n",
      "Epoch: 12 Step: 4950 Val Loss: 1.2845228910446167\n",
      "Epoch: 13 Step: 4975 Val Loss: 1.2880691289901733\n",
      "Epoch: 13 Step: 5000 Val Loss: 1.2875874042510986\n",
      "Epoch: 13 Step: 5025 Val Loss: 1.2875337600708008\n",
      "Epoch: 13 Step: 5050 Val Loss: 1.2828291654586792\n",
      "Epoch: 13 Step: 5075 Val Loss: 1.2906841039657593\n",
      "Epoch: 13 Step: 5100 Val Loss: 1.2854670286178589\n",
      "Epoch: 13 Step: 5125 Val Loss: 1.286227822303772\n",
      "Epoch: 13 Step: 5150 Val Loss: 1.2831591367721558\n",
      "Epoch: 13 Step: 5175 Val Loss: 1.2841596603393555\n",
      "Epoch: 13 Step: 5200 Val Loss: 1.2839012145996094\n",
      "Epoch: 13 Step: 5225 Val Loss: 1.285162091255188\n",
      "Epoch: 13 Step: 5250 Val Loss: 1.283656120300293\n",
      "Epoch: 13 Step: 5275 Val Loss: 1.2822681665420532\n",
      "Epoch: 13 Step: 5300 Val Loss: 1.2830532789230347\n",
      "Epoch: 13 Step: 5325 Val Loss: 1.285272240638733\n",
      "Epoch: 14 Step: 5350 Val Loss: 1.2835084199905396\n",
      "Epoch: 14 Step: 5375 Val Loss: 1.2827942371368408\n",
      "Epoch: 14 Step: 5400 Val Loss: 1.2838267087936401\n",
      "Epoch: 14 Step: 5425 Val Loss: 1.282119631767273\n",
      "Epoch: 14 Step: 5450 Val Loss: 1.2781949043273926\n",
      "Epoch: 14 Step: 5475 Val Loss: 1.2830700874328613\n",
      "Epoch: 14 Step: 5500 Val Loss: 1.2794313430786133\n",
      "Epoch: 14 Step: 5525 Val Loss: 1.2790838479995728\n",
      "Epoch: 14 Step: 5550 Val Loss: 1.2790223360061646\n",
      "Epoch: 14 Step: 5575 Val Loss: 1.281700611114502\n",
      "Epoch: 14 Step: 5600 Val Loss: 1.2835094928741455\n",
      "Epoch: 14 Step: 5625 Val Loss: 1.2798577547073364\n",
      "Epoch: 14 Step: 5650 Val Loss: 1.2774913311004639\n",
      "Epoch: 14 Step: 5675 Val Loss: 1.275557279586792\n",
      "Epoch: 14 Step: 5700 Val Loss: 1.2781741619110107\n",
      "Epoch: 14 Step: 5725 Val Loss: 1.2781018018722534\n",
      "Epoch: 15 Step: 5750 Val Loss: 1.278326153755188\n",
      "Epoch: 15 Step: 5775 Val Loss: 1.2774574756622314\n",
      "Epoch: 15 Step: 5800 Val Loss: 1.2756545543670654\n",
      "Epoch: 15 Step: 5825 Val Loss: 1.271590232849121\n",
      "Epoch: 15 Step: 5850 Val Loss: 1.2765253782272339\n",
      "Epoch: 15 Step: 5875 Val Loss: 1.278364896774292\n",
      "Epoch: 15 Step: 5900 Val Loss: 1.2779921293258667\n",
      "Epoch: 15 Step: 5925 Val Loss: 1.2781901359558105\n",
      "Epoch: 15 Step: 5950 Val Loss: 1.275803804397583\n",
      "Epoch: 15 Step: 5975 Val Loss: 1.2758755683898926\n",
      "Epoch: 15 Step: 6000 Val Loss: 1.2819185256958008\n",
      "Epoch: 15 Step: 6025 Val Loss: 1.2794790267944336\n",
      "Epoch: 15 Step: 6050 Val Loss: 1.2718883752822876\n",
      "Epoch: 15 Step: 6075 Val Loss: 1.27224862575531\n",
      "Epoch: 15 Step: 6100 Val Loss: 1.2763376235961914\n",
      "Epoch: 16 Step: 6125 Val Loss: 1.2789751291275024\n",
      "Epoch: 16 Step: 6150 Val Loss: 1.2808054685592651\n",
      "Epoch: 16 Step: 6175 Val Loss: 1.2771039009094238\n",
      "Epoch: 16 Step: 6200 Val Loss: 1.2761456966400146\n",
      "Epoch: 16 Step: 6225 Val Loss: 1.2788337469100952\n",
      "Epoch: 16 Step: 6250 Val Loss: 1.2746782302856445\n",
      "Epoch: 16 Step: 6275 Val Loss: 1.272855281829834\n",
      "Epoch: 16 Step: 6300 Val Loss: 1.2727117538452148\n",
      "Epoch: 16 Step: 6325 Val Loss: 1.277215600013733\n",
      "Epoch: 16 Step: 6350 Val Loss: 1.2765299081802368\n",
      "Epoch: 16 Step: 6375 Val Loss: 1.278788447380066\n",
      "Epoch: 16 Step: 6400 Val Loss: 1.2723039388656616\n",
      "Epoch: 16 Step: 6425 Val Loss: 1.270967721939087\n",
      "Epoch: 16 Step: 6450 Val Loss: 1.2730040550231934\n",
      "Epoch: 16 Step: 6475 Val Loss: 1.2737139463424683\n",
      "Epoch: 17 Step: 6500 Val Loss: 1.2790812253952026\n",
      "Epoch: 17 Step: 6525 Val Loss: 1.2735501527786255\n",
      "Epoch: 17 Step: 6550 Val Loss: 1.2794857025146484\n",
      "Epoch: 17 Step: 6575 Val Loss: 1.275392770767212\n",
      "Epoch: 17 Step: 6600 Val Loss: 1.2715541124343872\n",
      "Epoch: 17 Step: 6625 Val Loss: 1.2737901210784912\n",
      "Epoch: 17 Step: 6650 Val Loss: 1.2702412605285645\n",
      "Epoch: 17 Step: 6675 Val Loss: 1.273950219154358\n",
      "Epoch: 17 Step: 6700 Val Loss: 1.2714906930923462\n",
      "Epoch: 17 Step: 6725 Val Loss: 1.276533603668213\n",
      "Epoch: 17 Step: 6750 Val Loss: 1.2777858972549438\n",
      "Epoch: 17 Step: 6775 Val Loss: 1.2753065824508667\n",
      "Epoch: 17 Step: 6800 Val Loss: 1.2757046222686768\n",
      "Epoch: 17 Step: 6825 Val Loss: 1.2713466882705688\n",
      "Epoch: 17 Step: 6850 Val Loss: 1.2734959125518799\n",
      "Epoch: 17 Step: 6875 Val Loss: 1.273869276046753\n",
      "Epoch: 18 Step: 6900 Val Loss: 1.274102807044983\n",
      "Epoch: 18 Step: 6925 Val Loss: 1.2721916437149048\n",
      "Epoch: 18 Step: 6950 Val Loss: 1.270241141319275\n",
      "Epoch: 18 Step: 6975 Val Loss: 1.2702429294586182\n",
      "Epoch: 18 Step: 7000 Val Loss: 1.2731257677078247\n",
      "Epoch: 18 Step: 7025 Val Loss: 1.2702854871749878\n",
      "Epoch: 18 Step: 7050 Val Loss: 1.2740668058395386\n",
      "Epoch: 18 Step: 7075 Val Loss: 1.268109917640686\n",
      "Epoch: 18 Step: 7100 Val Loss: 1.2722392082214355\n",
      "Epoch: 18 Step: 7125 Val Loss: 1.272282361984253\n",
      "Epoch: 18 Step: 7150 Val Loss: 1.2729569673538208\n",
      "Epoch: 18 Step: 7175 Val Loss: 1.2716147899627686\n",
      "Epoch: 18 Step: 7200 Val Loss: 1.2679136991500854\n",
      "Epoch: 18 Step: 7225 Val Loss: 1.2722305059432983\n",
      "Epoch: 18 Step: 7250 Val Loss: 1.2657479047775269\n",
      "Epoch: 19 Step: 7275 Val Loss: 1.2729918956756592\n",
      "Epoch: 19 Step: 7300 Val Loss: 1.2725931406021118\n",
      "Epoch: 19 Step: 7325 Val Loss: 1.2689322233200073\n",
      "Epoch: 19 Step: 7350 Val Loss: 1.2702502012252808\n",
      "Epoch: 19 Step: 7375 Val Loss: 1.2713732719421387\n",
      "Epoch: 19 Step: 7400 Val Loss: 1.265557885169983\n",
      "Epoch: 19 Step: 7425 Val Loss: 1.2654376029968262\n",
      "Epoch: 19 Step: 7450 Val Loss: 1.2647168636322021\n",
      "Epoch: 19 Step: 7475 Val Loss: 1.2663440704345703\n",
      "Epoch: 19 Step: 7500 Val Loss: 1.2688894271850586\n",
      "Epoch: 19 Step: 7525 Val Loss: 1.2692134380340576\n",
      "Epoch: 19 Step: 7550 Val Loss: 1.2693507671356201\n",
      "Epoch: 19 Step: 7575 Val Loss: 1.2695552110671997\n",
      "Epoch: 19 Step: 7600 Val Loss: 1.2692588567733765\n",
      "Epoch: 19 Step: 7625 Val Loss: 1.2654906511306763\n",
      "Epoch: 20 Step: 7650 Val Loss: 1.2662714719772339\n",
      "Epoch: 20 Step: 7675 Val Loss: 1.2683324813842773\n",
      "Epoch: 20 Step: 7700 Val Loss: 1.2702594995498657\n",
      "Epoch: 20 Step: 7725 Val Loss: 1.2725920677185059\n",
      "Epoch: 20 Step: 7750 Val Loss: 1.2691997289657593\n",
      "Epoch: 20 Step: 7775 Val Loss: 1.2678101062774658\n",
      "Epoch: 20 Step: 7800 Val Loss: 1.2656638622283936\n",
      "Epoch: 20 Step: 7825 Val Loss: 1.2663296461105347\n",
      "Epoch: 20 Step: 7850 Val Loss: 1.2682936191558838\n",
      "Epoch: 20 Step: 7875 Val Loss: 1.2703304290771484\n",
      "Epoch: 20 Step: 7900 Val Loss: 1.2765886783599854\n",
      "Epoch: 20 Step: 7925 Val Loss: 1.270756721496582\n",
      "Epoch: 20 Step: 7950 Val Loss: 1.2686923742294312\n",
      "Epoch: 20 Step: 7975 Val Loss: 1.269008994102478\n",
      "Epoch: 20 Step: 8000 Val Loss: 1.2687925100326538\n",
      "Epoch: 21 Step: 8025 Val Loss: 1.2724976539611816\n",
      "Epoch: 21 Step: 8050 Val Loss: 1.2724671363830566\n",
      "Epoch: 21 Step: 8075 Val Loss: 1.26982581615448\n",
      "Epoch: 21 Step: 8100 Val Loss: 1.267472267150879\n",
      "Epoch: 21 Step: 8125 Val Loss: 1.2650867700576782\n",
      "Epoch: 21 Step: 8150 Val Loss: 1.267498254776001\n",
      "Epoch: 21 Step: 8175 Val Loss: 1.264776349067688\n",
      "Epoch: 21 Step: 8200 Val Loss: 1.2647804021835327\n",
      "Epoch: 21 Step: 8225 Val Loss: 1.2667946815490723\n",
      "Epoch: 21 Step: 8250 Val Loss: 1.2681740522384644\n",
      "Epoch: 21 Step: 8275 Val Loss: 1.270154356956482\n",
      "Epoch: 21 Step: 8300 Val Loss: 1.269321084022522\n",
      "Epoch: 21 Step: 8325 Val Loss: 1.2676604986190796\n",
      "Epoch: 21 Step: 8350 Val Loss: 1.2608957290649414\n",
      "Epoch: 21 Step: 8375 Val Loss: 1.2701472043991089\n",
      "Epoch: 21 Step: 8400 Val Loss: 1.260342001914978\n",
      "Epoch: 22 Step: 8425 Val Loss: 1.2650831937789917\n",
      "Epoch: 22 Step: 8450 Val Loss: 1.2676485776901245\n",
      "Epoch: 22 Step: 8475 Val Loss: 1.2680740356445312\n",
      "Epoch: 22 Step: 8500 Val Loss: 1.2610152959823608\n",
      "Epoch: 22 Step: 8525 Val Loss: 1.2606134414672852\n",
      "Epoch: 22 Step: 8550 Val Loss: 1.2632757425308228\n",
      "Epoch: 22 Step: 8575 Val Loss: 1.2588520050048828\n",
      "Epoch: 22 Step: 8600 Val Loss: 1.2655749320983887\n",
      "Epoch: 22 Step: 8625 Val Loss: 1.262129783630371\n",
      "Epoch: 22 Step: 8650 Val Loss: 1.2637687921524048\n",
      "Epoch: 22 Step: 8675 Val Loss: 1.2671856880187988\n",
      "Epoch: 22 Step: 8700 Val Loss: 1.2699918746948242\n",
      "Epoch: 22 Step: 8725 Val Loss: 1.2626150846481323\n",
      "Epoch: 22 Step: 8750 Val Loss: 1.2650011777877808\n",
      "Epoch: 22 Step: 8775 Val Loss: 1.2628109455108643\n",
      "Epoch: 23 Step: 8800 Val Loss: 1.2621079683303833\n",
      "Epoch: 23 Step: 8825 Val Loss: 1.2667332887649536\n",
      "Epoch: 23 Step: 8850 Val Loss: 1.2641258239746094\n",
      "Epoch: 23 Step: 8875 Val Loss: 1.2632429599761963\n",
      "Epoch: 23 Step: 8900 Val Loss: 1.267116904258728\n",
      "Epoch: 23 Step: 8925 Val Loss: 1.2618200778961182\n",
      "Epoch: 23 Step: 8950 Val Loss: 1.2635291814804077\n",
      "Epoch: 23 Step: 8975 Val Loss: 1.2602077722549438\n",
      "Epoch: 23 Step: 9000 Val Loss: 1.2589722871780396\n",
      "Epoch: 23 Step: 9025 Val Loss: 1.259373664855957\n",
      "Epoch: 23 Step: 9050 Val Loss: 1.262810468673706\n",
      "Epoch: 23 Step: 9075 Val Loss: 1.263123631477356\n",
      "Epoch: 23 Step: 9100 Val Loss: 1.2588083744049072\n",
      "Epoch: 23 Step: 9125 Val Loss: 1.2622056007385254\n",
      "Epoch: 23 Step: 9150 Val Loss: 1.2670809030532837\n",
      "Epoch: 24 Step: 9175 Val Loss: 1.2651078701019287\n",
      "Epoch: 24 Step: 9200 Val Loss: 1.2662922143936157\n",
      "Epoch: 24 Step: 9225 Val Loss: 1.2686166763305664\n",
      "Epoch: 24 Step: 9250 Val Loss: 1.2629181146621704\n",
      "Epoch: 24 Step: 9275 Val Loss: 1.2640788555145264\n",
      "Epoch: 24 Step: 9300 Val Loss: 1.2636418342590332\n",
      "Epoch: 24 Step: 9325 Val Loss: 1.26044762134552\n",
      "Epoch: 24 Step: 9350 Val Loss: 1.2651793956756592\n",
      "Epoch: 24 Step: 9375 Val Loss: 1.2644437551498413\n",
      "Epoch: 24 Step: 9400 Val Loss: 1.2633684873580933\n",
      "Epoch: 24 Step: 9425 Val Loss: 1.2634742259979248\n",
      "Epoch: 24 Step: 9450 Val Loss: 1.2672878503799438\n",
      "Epoch: 24 Step: 9475 Val Loss: 1.2644151449203491\n",
      "Epoch: 24 Step: 9500 Val Loss: 1.262737512588501\n",
      "Epoch: 24 Step: 9525 Val Loss: 1.2609679698944092\n",
      "Epoch: 24 Step: 9550 Val Loss: 1.2599072456359863\n",
      "Epoch: 25 Step: 9575 Val Loss: 1.2610507011413574\n",
      "Epoch: 25 Step: 9600 Val Loss: 1.259879469871521\n",
      "Epoch: 25 Step: 9625 Val Loss: 1.2604129314422607\n",
      "Epoch: 25 Step: 9650 Val Loss: 1.2584123611450195\n",
      "Epoch: 25 Step: 9675 Val Loss: 1.259774923324585\n",
      "Epoch: 25 Step: 9700 Val Loss: 1.2614903450012207\n",
      "Epoch: 25 Step: 9725 Val Loss: 1.2612906694412231\n",
      "Epoch: 25 Step: 9750 Val Loss: 1.2632654905319214\n",
      "Epoch: 25 Step: 9775 Val Loss: 1.2635283470153809\n",
      "Epoch: 25 Step: 9800 Val Loss: 1.260054588317871\n",
      "Epoch: 25 Step: 9825 Val Loss: 1.2643978595733643\n",
      "Epoch: 25 Step: 9850 Val Loss: 1.2624528408050537\n",
      "Epoch: 25 Step: 9875 Val Loss: 1.2592309713363647\n",
      "Epoch: 25 Step: 9900 Val Loss: 1.2629189491271973\n",
      "Epoch: 25 Step: 9925 Val Loss: 1.262049913406372\n",
      "Epoch: 26 Step: 9950 Val Loss: 1.2647596597671509\n",
      "Epoch: 26 Step: 9975 Val Loss: 1.261959195137024\n",
      "Epoch: 26 Step: 10000 Val Loss: 1.258835792541504\n",
      "Epoch: 26 Step: 10025 Val Loss: 1.2611675262451172\n",
      "Epoch: 26 Step: 10050 Val Loss: 1.2627933025360107\n",
      "Epoch: 26 Step: 10075 Val Loss: 1.2612215280532837\n",
      "Epoch: 26 Step: 10100 Val Loss: 1.258854627609253\n",
      "Epoch: 26 Step: 10125 Val Loss: 1.2628544569015503\n",
      "Epoch: 26 Step: 10150 Val Loss: 1.258578896522522\n",
      "Epoch: 26 Step: 10175 Val Loss: 1.2640137672424316\n",
      "Epoch: 26 Step: 10200 Val Loss: 1.264267921447754\n",
      "Epoch: 26 Step: 10225 Val Loss: 1.2638578414916992\n",
      "Epoch: 26 Step: 10250 Val Loss: 1.258477807044983\n",
      "Epoch: 26 Step: 10275 Val Loss: 1.2580528259277344\n",
      "Epoch: 26 Step: 10300 Val Loss: 1.2604336738586426\n",
      "Epoch: 27 Step: 10325 Val Loss: 1.2586265802383423\n",
      "Epoch: 27 Step: 10350 Val Loss: 1.2624402046203613\n",
      "Epoch: 27 Step: 10375 Val Loss: 1.2630224227905273\n",
      "Epoch: 27 Step: 10400 Val Loss: 1.2592744827270508\n",
      "Epoch: 27 Step: 10425 Val Loss: 1.2588592767715454\n",
      "Epoch: 27 Step: 10450 Val Loss: 1.2592872381210327\n",
      "Epoch: 27 Step: 10475 Val Loss: 1.2595243453979492\n",
      "Epoch: 27 Step: 10500 Val Loss: 1.255354881286621\n",
      "Epoch: 27 Step: 10525 Val Loss: 1.2576311826705933\n",
      "Epoch: 27 Step: 10550 Val Loss: 1.260106086730957\n",
      "Epoch: 27 Step: 10575 Val Loss: 1.2644790410995483\n",
      "Epoch: 27 Step: 10600 Val Loss: 1.2620720863342285\n",
      "Epoch: 27 Step: 10625 Val Loss: 1.257370114326477\n",
      "Epoch: 27 Step: 10650 Val Loss: 1.2589995861053467\n",
      "Epoch: 27 Step: 10675 Val Loss: 1.2621902227401733\n",
      "Epoch: 28 Step: 10700 Val Loss: 1.2600361108779907\n",
      "Epoch: 28 Step: 10725 Val Loss: 1.2617875337600708\n",
      "Epoch: 28 Step: 10750 Val Loss: 1.2611937522888184\n",
      "Epoch: 28 Step: 10775 Val Loss: 1.2621335983276367\n",
      "Epoch: 28 Step: 10800 Val Loss: 1.2641901969909668\n",
      "Epoch: 28 Step: 10825 Val Loss: 1.2587215900421143\n",
      "Epoch: 28 Step: 10850 Val Loss: 1.2589393854141235\n",
      "Epoch: 28 Step: 10875 Val Loss: 1.260560154914856\n",
      "Epoch: 28 Step: 10900 Val Loss: 1.25797700881958\n",
      "Epoch: 28 Step: 10925 Val Loss: 1.2584811449050903\n",
      "Epoch: 28 Step: 10950 Val Loss: 1.2624021768569946\n",
      "Epoch: 28 Step: 10975 Val Loss: 1.2587567567825317\n",
      "Epoch: 28 Step: 11000 Val Loss: 1.257043480873108\n",
      "Epoch: 28 Step: 11025 Val Loss: 1.2540239095687866\n",
      "Epoch: 28 Step: 11050 Val Loss: 1.2595900297164917\n",
      "Epoch: 28 Step: 11075 Val Loss: 1.2538871765136719\n",
      "Epoch: 29 Step: 11100 Val Loss: 1.2557233572006226\n",
      "Epoch: 29 Step: 11125 Val Loss: 1.2530683279037476\n",
      "Epoch: 29 Step: 11150 Val Loss: 1.255662441253662\n",
      "Epoch: 29 Step: 11175 Val Loss: 1.2551358938217163\n",
      "Epoch: 29 Step: 11200 Val Loss: 1.2577656507492065\n",
      "Epoch: 29 Step: 11225 Val Loss: 1.2590742111206055\n",
      "Epoch: 29 Step: 11250 Val Loss: 1.24993896484375\n",
      "Epoch: 29 Step: 11275 Val Loss: 1.2544915676116943\n",
      "Epoch: 29 Step: 11300 Val Loss: 1.2559711933135986\n",
      "Epoch: 29 Step: 11325 Val Loss: 1.255602478981018\n",
      "Epoch: 29 Step: 11350 Val Loss: 1.2597333192825317\n",
      "Epoch: 29 Step: 11375 Val Loss: 1.260323405265808\n",
      "Epoch: 29 Step: 11400 Val Loss: 1.2556509971618652\n",
      "Epoch: 29 Step: 11425 Val Loss: 1.2572613954544067\n",
      "Epoch: 29 Step: 11450 Val Loss: 1.2519336938858032\n",
      "Epoch: 30 Step: 11475 Val Loss: 1.2544035911560059\n",
      "Epoch: 30 Step: 11500 Val Loss: 1.2562179565429688\n",
      "Epoch: 30 Step: 11525 Val Loss: 1.253127098083496\n",
      "Epoch: 30 Step: 11550 Val Loss: 1.2522870302200317\n",
      "Epoch: 30 Step: 11575 Val Loss: 1.2508996725082397\n",
      "Epoch: 30 Step: 11600 Val Loss: 1.2548954486846924\n",
      "Epoch: 30 Step: 11625 Val Loss: 1.2583606243133545\n",
      "Epoch: 30 Step: 11650 Val Loss: 1.2506511211395264\n",
      "Epoch: 30 Step: 11675 Val Loss: 1.2534308433532715\n",
      "Epoch: 30 Step: 11700 Val Loss: 1.2560861110687256\n",
      "Epoch: 30 Step: 11725 Val Loss: 1.2527927160263062\n",
      "Epoch: 30 Step: 11750 Val Loss: 1.2542977333068848\n",
      "Epoch: 30 Step: 11775 Val Loss: 1.251990556716919\n",
      "Epoch: 30 Step: 11800 Val Loss: 1.2505213022232056\n",
      "Epoch: 30 Step: 11825 Val Loss: 1.2565858364105225\n",
      "Epoch: 31 Step: 11850 Val Loss: 1.254396677017212\n",
      "Epoch: 31 Step: 11875 Val Loss: 1.2576775550842285\n",
      "Epoch: 31 Step: 11900 Val Loss: 1.2580245733261108\n",
      "Epoch: 31 Step: 11925 Val Loss: 1.2563289403915405\n",
      "Epoch: 31 Step: 11950 Val Loss: 1.2574576139450073\n",
      "Epoch: 31 Step: 11975 Val Loss: 1.2578706741333008\n",
      "Epoch: 31 Step: 12000 Val Loss: 1.2542937994003296\n",
      "Epoch: 31 Step: 12025 Val Loss: 1.2504690885543823\n",
      "Epoch: 31 Step: 12050 Val Loss: 1.2542003393173218\n",
      "Epoch: 31 Step: 12075 Val Loss: 1.2528337240219116\n",
      "Epoch: 31 Step: 12100 Val Loss: 1.2545689344406128\n",
      "Epoch: 31 Step: 12125 Val Loss: 1.2551205158233643\n",
      "Epoch: 31 Step: 12150 Val Loss: 1.2525169849395752\n",
      "Epoch: 31 Step: 12175 Val Loss: 1.2522778511047363\n",
      "Epoch: 31 Step: 12200 Val Loss: 1.2501764297485352\n",
      "Epoch: 32 Step: 12225 Val Loss: 1.2557787895202637\n",
      "Epoch: 32 Step: 12250 Val Loss: 1.2568728923797607\n",
      "Epoch: 32 Step: 12275 Val Loss: 1.2562801837921143\n",
      "Epoch: 32 Step: 12300 Val Loss: 1.2525525093078613\n",
      "Epoch: 32 Step: 12325 Val Loss: 1.2546085119247437\n",
      "Epoch: 32 Step: 12350 Val Loss: 1.253174066543579\n",
      "Epoch: 32 Step: 12375 Val Loss: 1.2537153959274292\n",
      "Epoch: 32 Step: 12400 Val Loss: 1.2531111240386963\n",
      "Epoch: 32 Step: 12425 Val Loss: 1.2547154426574707\n",
      "Epoch: 32 Step: 12450 Val Loss: 1.2552927732467651\n",
      "Epoch: 32 Step: 12475 Val Loss: 1.2518829107284546\n",
      "Epoch: 32 Step: 12500 Val Loss: 1.253176212310791\n",
      "Epoch: 32 Step: 12525 Val Loss: 1.2561390399932861\n",
      "Epoch: 32 Step: 12550 Val Loss: 1.2525908946990967\n",
      "Epoch: 32 Step: 12575 Val Loss: 1.2553497552871704\n",
      "Epoch: 32 Step: 12600 Val Loss: 1.2492904663085938\n",
      "Epoch: 33 Step: 12625 Val Loss: 1.253283977508545\n",
      "Epoch: 33 Step: 12650 Val Loss: 1.2556326389312744\n",
      "Epoch: 33 Step: 12675 Val Loss: 1.2543150186538696\n",
      "Epoch: 33 Step: 12700 Val Loss: 1.25583815574646\n",
      "Epoch: 33 Step: 12725 Val Loss: 1.2579419612884521\n",
      "Epoch: 33 Step: 12750 Val Loss: 1.25745689868927\n",
      "Epoch: 33 Step: 12775 Val Loss: 1.2552670240402222\n",
      "Epoch: 33 Step: 12800 Val Loss: 1.2574363946914673\n",
      "Epoch: 33 Step: 12825 Val Loss: 1.2558131217956543\n",
      "Epoch: 33 Step: 12850 Val Loss: 1.2529897689819336\n",
      "Epoch: 33 Step: 12875 Val Loss: 1.2588614225387573\n",
      "Epoch: 33 Step: 12900 Val Loss: 1.2610044479370117\n",
      "Epoch: 33 Step: 12925 Val Loss: 1.253371238708496\n",
      "Epoch: 33 Step: 12950 Val Loss: 1.2588521242141724\n",
      "Epoch: 33 Step: 12975 Val Loss: 1.2574456930160522\n",
      "Epoch: 34 Step: 13000 Val Loss: 1.2580279111862183\n",
      "Epoch: 34 Step: 13025 Val Loss: 1.2598388195037842\n",
      "Epoch: 34 Step: 13050 Val Loss: 1.2624891996383667\n",
      "Epoch: 34 Step: 13075 Val Loss: 1.2560878992080688\n",
      "Epoch: 34 Step: 13100 Val Loss: 1.2616857290267944\n",
      "Epoch: 34 Step: 13125 Val Loss: 1.2593798637390137\n",
      "Epoch: 34 Step: 13150 Val Loss: 1.2534756660461426\n",
      "Epoch: 34 Step: 13175 Val Loss: 1.2546930313110352\n",
      "Epoch: 34 Step: 13200 Val Loss: 1.2571214437484741\n",
      "Epoch: 34 Step: 13225 Val Loss: 1.2543346881866455\n",
      "Epoch: 34 Step: 13250 Val Loss: 1.2561761140823364\n",
      "Epoch: 34 Step: 13275 Val Loss: 1.2567542791366577\n",
      "Epoch: 34 Step: 13300 Val Loss: 1.258744716644287\n",
      "Epoch: 34 Step: 13325 Val Loss: 1.2600648403167725\n",
      "Epoch: 34 Step: 13350 Val Loss: 1.256287932395935\n",
      "Epoch: 35 Step: 13375 Val Loss: 1.254874587059021\n",
      "Epoch: 35 Step: 13400 Val Loss: 1.2539616823196411\n",
      "Epoch: 35 Step: 13425 Val Loss: 1.2563129663467407\n",
      "Epoch: 35 Step: 13450 Val Loss: 1.2552762031555176\n",
      "Epoch: 35 Step: 13475 Val Loss: 1.254437804222107\n",
      "Epoch: 35 Step: 13500 Val Loss: 1.2537719011306763\n",
      "Epoch: 35 Step: 13525 Val Loss: 1.2555365562438965\n",
      "Epoch: 35 Step: 13550 Val Loss: 1.256590485572815\n",
      "Epoch: 35 Step: 13575 Val Loss: 1.2551565170288086\n",
      "Epoch: 35 Step: 13600 Val Loss: 1.257033109664917\n",
      "Epoch: 35 Step: 13625 Val Loss: 1.2596774101257324\n",
      "Epoch: 35 Step: 13650 Val Loss: 1.259785771369934\n",
      "Epoch: 35 Step: 13675 Val Loss: 1.2589820623397827\n",
      "Epoch: 35 Step: 13700 Val Loss: 1.2519954442977905\n",
      "Epoch: 35 Step: 13725 Val Loss: 1.258313775062561\n",
      "Epoch: 35 Step: 13750 Val Loss: 1.2550835609436035\n",
      "Epoch: 36 Step: 13775 Val Loss: 1.2585567235946655\n",
      "Epoch: 36 Step: 13800 Val Loss: 1.2546892166137695\n",
      "Epoch: 36 Step: 13825 Val Loss: 1.255780577659607\n",
      "Epoch: 36 Step: 13850 Val Loss: 1.252388834953308\n",
      "Epoch: 36 Step: 13875 Val Loss: 1.2542442083358765\n",
      "Epoch: 36 Step: 13900 Val Loss: 1.2522095441818237\n",
      "Epoch: 36 Step: 13925 Val Loss: 1.252199411392212\n",
      "Epoch: 36 Step: 13950 Val Loss: 1.2518311738967896\n",
      "Epoch: 36 Step: 13975 Val Loss: 1.2468434572219849\n",
      "Epoch: 36 Step: 14000 Val Loss: 1.250612735748291\n",
      "Epoch: 36 Step: 14025 Val Loss: 1.255234718322754\n",
      "Epoch: 36 Step: 14050 Val Loss: 1.2581530809402466\n",
      "Epoch: 36 Step: 14075 Val Loss: 1.2543503046035767\n",
      "Epoch: 36 Step: 14100 Val Loss: 1.2588673830032349\n",
      "Epoch: 36 Step: 14125 Val Loss: 1.2545678615570068\n",
      "Epoch: 37 Step: 14150 Val Loss: 1.2571346759796143\n",
      "Epoch: 37 Step: 14175 Val Loss: 1.2528382539749146\n",
      "Epoch: 37 Step: 14200 Val Loss: 1.2510820627212524\n",
      "Epoch: 37 Step: 14225 Val Loss: 1.2538230419158936\n",
      "Epoch: 37 Step: 14250 Val Loss: 1.2582858800888062\n",
      "Epoch: 37 Step: 14275 Val Loss: 1.2543071508407593\n",
      "Epoch: 37 Step: 14300 Val Loss: 1.2555599212646484\n",
      "Epoch: 37 Step: 14325 Val Loss: 1.2518254518508911\n",
      "Epoch: 37 Step: 14350 Val Loss: 1.252130150794983\n",
      "Epoch: 37 Step: 14375 Val Loss: 1.2499638795852661\n",
      "Epoch: 37 Step: 14400 Val Loss: 1.2510652542114258\n",
      "Epoch: 37 Step: 14425 Val Loss: 1.2528104782104492\n",
      "Epoch: 37 Step: 14450 Val Loss: 1.2523860931396484\n",
      "Epoch: 37 Step: 14475 Val Loss: 1.2538363933563232\n",
      "Epoch: 37 Step: 14500 Val Loss: 1.2578158378601074\n",
      "Epoch: 38 Step: 14525 Val Loss: 1.2504502534866333\n",
      "Epoch: 38 Step: 14550 Val Loss: 1.2531170845031738\n",
      "Epoch: 38 Step: 14575 Val Loss: 1.2543772459030151\n",
      "Epoch: 38 Step: 14600 Val Loss: 1.250486969947815\n",
      "Epoch: 38 Step: 14625 Val Loss: 1.254238247871399\n",
      "Epoch: 38 Step: 14650 Val Loss: 1.2561672925949097\n",
      "Epoch: 38 Step: 14675 Val Loss: 1.2524787187576294\n",
      "Epoch: 38 Step: 14700 Val Loss: 1.2546851634979248\n",
      "Epoch: 38 Step: 14725 Val Loss: 1.2550891637802124\n",
      "Epoch: 38 Step: 14750 Val Loss: 1.2534284591674805\n",
      "Epoch: 38 Step: 14775 Val Loss: 1.2570933103561401\n",
      "Epoch: 38 Step: 14800 Val Loss: 1.2586591243743896\n",
      "Epoch: 38 Step: 14825 Val Loss: 1.2606395483016968\n",
      "Epoch: 38 Step: 14850 Val Loss: 1.2589256763458252\n",
      "Epoch: 38 Step: 14875 Val Loss: 1.2560564279556274\n",
      "Epoch: 39 Step: 14900 Val Loss: 1.2552497386932373\n",
      "Epoch: 39 Step: 14925 Val Loss: 1.251886248588562\n",
      "Epoch: 39 Step: 14950 Val Loss: 1.252150535583496\n",
      "Epoch: 39 Step: 14975 Val Loss: 1.2554877996444702\n",
      "Epoch: 39 Step: 15000 Val Loss: 1.253145694732666\n",
      "Epoch: 39 Step: 15025 Val Loss: 1.2559319734573364\n",
      "Epoch: 39 Step: 15050 Val Loss: 1.2564127445220947\n",
      "Epoch: 39 Step: 15075 Val Loss: 1.2538816928863525\n",
      "Epoch: 39 Step: 15100 Val Loss: 1.2526888847351074\n",
      "Epoch: 39 Step: 15125 Val Loss: 1.252663016319275\n",
      "Epoch: 39 Step: 15150 Val Loss: 1.247766375541687\n",
      "Epoch: 39 Step: 15175 Val Loss: 1.2498652935028076\n",
      "Epoch: 39 Step: 15200 Val Loss: 1.2539957761764526\n",
      "Epoch: 39 Step: 15225 Val Loss: 1.2508453130722046\n",
      "Epoch: 39 Step: 15250 Val Loss: 1.2531459331512451\n",
      "Epoch: 39 Step: 15275 Val Loss: 1.2519015073776245\n",
      "Epoch: 40 Step: 15300 Val Loss: 1.250614047050476\n",
      "Epoch: 40 Step: 15325 Val Loss: 1.2502168416976929\n",
      "Epoch: 40 Step: 15350 Val Loss: 1.2461025714874268\n",
      "Epoch: 40 Step: 15375 Val Loss: 1.2470178604125977\n",
      "Epoch: 40 Step: 15400 Val Loss: 1.2483274936676025\n",
      "Epoch: 40 Step: 15425 Val Loss: 1.248081922531128\n",
      "Epoch: 40 Step: 15450 Val Loss: 1.2526657581329346\n",
      "Epoch: 40 Step: 15475 Val Loss: 1.2534857988357544\n",
      "Epoch: 40 Step: 15500 Val Loss: 1.2465693950653076\n",
      "Epoch: 40 Step: 15525 Val Loss: 1.2487255334854126\n",
      "Epoch: 40 Step: 15550 Val Loss: 1.251480221748352\n",
      "Epoch: 40 Step: 15575 Val Loss: 1.255052924156189\n",
      "Epoch: 40 Step: 15600 Val Loss: 1.2481229305267334\n",
      "Epoch: 40 Step: 15625 Val Loss: 1.2476645708084106\n",
      "Epoch: 40 Step: 15650 Val Loss: 1.2543010711669922\n",
      "Epoch: 41 Step: 15675 Val Loss: 1.2499573230743408\n",
      "Epoch: 41 Step: 15700 Val Loss: 1.2523229122161865\n",
      "Epoch: 41 Step: 15725 Val Loss: 1.2491778135299683\n",
      "Epoch: 41 Step: 15750 Val Loss: 1.2501121759414673\n",
      "Epoch: 41 Step: 15775 Val Loss: 1.2529869079589844\n",
      "Epoch: 41 Step: 15800 Val Loss: 1.2547146081924438\n",
      "Epoch: 41 Step: 15825 Val Loss: 1.249333381652832\n",
      "Epoch: 41 Step: 15850 Val Loss: 1.250606656074524\n",
      "Epoch: 41 Step: 15875 Val Loss: 1.2552833557128906\n",
      "Epoch: 41 Step: 15900 Val Loss: 1.2540004253387451\n",
      "Epoch: 41 Step: 15925 Val Loss: 1.2525922060012817\n",
      "Epoch: 41 Step: 15950 Val Loss: 1.2564635276794434\n",
      "Epoch: 41 Step: 15975 Val Loss: 1.2507164478302002\n",
      "Epoch: 41 Step: 16000 Val Loss: 1.2527769804000854\n",
      "Epoch: 41 Step: 16025 Val Loss: 1.2550854682922363\n",
      "Epoch: 42 Step: 16050 Val Loss: 1.257318139076233\n",
      "Epoch: 42 Step: 16075 Val Loss: 1.2542272806167603\n",
      "Epoch: 42 Step: 16100 Val Loss: 1.2543848752975464\n",
      "Epoch: 42 Step: 16125 Val Loss: 1.2507660388946533\n",
      "Epoch: 42 Step: 16150 Val Loss: 1.254024863243103\n",
      "Epoch: 42 Step: 16175 Val Loss: 1.2539697885513306\n",
      "Epoch: 42 Step: 16200 Val Loss: 1.2537416219711304\n",
      "Epoch: 42 Step: 16225 Val Loss: 1.2512282133102417\n",
      "Epoch: 42 Step: 16250 Val Loss: 1.2529867887496948\n",
      "Epoch: 42 Step: 16275 Val Loss: 1.251418113708496\n",
      "Epoch: 42 Step: 16300 Val Loss: 1.2531747817993164\n",
      "Epoch: 42 Step: 16325 Val Loss: 1.2542599439620972\n",
      "Epoch: 42 Step: 16350 Val Loss: 1.2546308040618896\n",
      "Epoch: 42 Step: 16375 Val Loss: 1.249536395072937\n",
      "Epoch: 42 Step: 16400 Val Loss: 1.2536078691482544\n",
      "Epoch: 42 Step: 16425 Val Loss: 1.252851963043213\n",
      "Epoch: 43 Step: 16450 Val Loss: 1.2584184408187866\n",
      "Epoch: 43 Step: 16475 Val Loss: 1.2550783157348633\n",
      "Epoch: 43 Step: 16500 Val Loss: 1.2475532293319702\n",
      "Epoch: 43 Step: 16525 Val Loss: 1.2523823976516724\n",
      "Epoch: 43 Step: 16550 Val Loss: 1.2575883865356445\n",
      "Epoch: 43 Step: 16575 Val Loss: 1.258924126625061\n",
      "Epoch: 43 Step: 16600 Val Loss: 1.2566379308700562\n",
      "Epoch: 43 Step: 16625 Val Loss: 1.257365345954895\n",
      "Epoch: 43 Step: 16650 Val Loss: 1.2498669624328613\n",
      "Epoch: 43 Step: 16675 Val Loss: 1.2501047849655151\n",
      "Epoch: 43 Step: 16700 Val Loss: 1.2525748014450073\n",
      "Epoch: 43 Step: 16725 Val Loss: 1.2557353973388672\n",
      "Epoch: 43 Step: 16750 Val Loss: 1.2520668506622314\n",
      "Epoch: 43 Step: 16775 Val Loss: 1.2509645223617554\n",
      "Epoch: 43 Step: 16800 Val Loss: 1.2502338886260986\n",
      "Epoch: 44 Step: 16825 Val Loss: 1.2531102895736694\n",
      "Epoch: 44 Step: 16850 Val Loss: 1.2542369365692139\n",
      "Epoch: 44 Step: 16875 Val Loss: 1.2545371055603027\n",
      "Epoch: 44 Step: 16900 Val Loss: 1.2552753686904907\n",
      "Epoch: 44 Step: 16925 Val Loss: 1.2544516324996948\n",
      "Epoch: 44 Step: 16950 Val Loss: 1.2522221803665161\n",
      "Epoch: 44 Step: 16975 Val Loss: 1.2528868913650513\n",
      "Epoch: 44 Step: 17000 Val Loss: 1.2524811029434204\n",
      "Epoch: 44 Step: 17025 Val Loss: 1.2507774829864502\n",
      "Epoch: 44 Step: 17050 Val Loss: 1.2519176006317139\n",
      "Epoch: 44 Step: 17075 Val Loss: 1.2527930736541748\n",
      "Epoch: 44 Step: 17100 Val Loss: 1.2540299892425537\n",
      "Epoch: 44 Step: 17125 Val Loss: 1.2516982555389404\n",
      "Epoch: 44 Step: 17150 Val Loss: 1.2503231763839722\n",
      "Epoch: 44 Step: 17175 Val Loss: 1.2506895065307617\n",
      "Epoch: 45 Step: 17200 Val Loss: 1.2487751245498657\n",
      "Epoch: 45 Step: 17225 Val Loss: 1.248399257659912\n",
      "Epoch: 45 Step: 17250 Val Loss: 1.2516467571258545\n",
      "Epoch: 45 Step: 17275 Val Loss: 1.2512458562850952\n",
      "Epoch: 45 Step: 17300 Val Loss: 1.2551499605178833\n",
      "Epoch: 45 Step: 17325 Val Loss: 1.2533433437347412\n",
      "Epoch: 45 Step: 17350 Val Loss: 1.2541470527648926\n",
      "Epoch: 45 Step: 17375 Val Loss: 1.2533345222473145\n",
      "Epoch: 45 Step: 17400 Val Loss: 1.257638931274414\n",
      "Epoch: 45 Step: 17425 Val Loss: 1.2566132545471191\n",
      "Epoch: 45 Step: 17450 Val Loss: 1.260013461112976\n",
      "Epoch: 45 Step: 17475 Val Loss: 1.2579870223999023\n",
      "Epoch: 45 Step: 17500 Val Loss: 1.2571325302124023\n",
      "Epoch: 45 Step: 17525 Val Loss: 1.253557562828064\n",
      "Epoch: 45 Step: 17550 Val Loss: 1.2555240392684937\n",
      "Epoch: 46 Step: 17575 Val Loss: 1.2567633390426636\n",
      "Epoch: 46 Step: 17600 Val Loss: 1.2502673864364624\n",
      "Epoch: 46 Step: 17625 Val Loss: 1.2544666528701782\n",
      "Epoch: 46 Step: 17650 Val Loss: 1.2541502714157104\n",
      "Epoch: 46 Step: 17675 Val Loss: 1.2548238039016724\n",
      "Epoch: 46 Step: 17700 Val Loss: 1.2560467720031738\n",
      "Epoch: 46 Step: 17725 Val Loss: 1.256486177444458\n",
      "Epoch: 46 Step: 17750 Val Loss: 1.2530678510665894\n",
      "Epoch: 46 Step: 17775 Val Loss: 1.255597472190857\n",
      "Epoch: 46 Step: 17800 Val Loss: 1.2534445524215698\n",
      "Epoch: 46 Step: 17825 Val Loss: 1.250304102897644\n",
      "Epoch: 46 Step: 17850 Val Loss: 1.2545368671417236\n",
      "Epoch: 46 Step: 17875 Val Loss: 1.2587864398956299\n",
      "Epoch: 46 Step: 17900 Val Loss: 1.2526774406433105\n",
      "Epoch: 46 Step: 17925 Val Loss: 1.2548549175262451\n",
      "Epoch: 46 Step: 17950 Val Loss: 1.248879313468933\n",
      "Epoch: 47 Step: 17975 Val Loss: 1.2552435398101807\n",
      "Epoch: 47 Step: 18000 Val Loss: 1.2513173818588257\n",
      "Epoch: 47 Step: 18025 Val Loss: 1.2519892454147339\n",
      "Epoch: 47 Step: 18050 Val Loss: 1.2530150413513184\n",
      "Epoch: 47 Step: 18075 Val Loss: 1.2550965547561646\n",
      "Epoch: 47 Step: 18100 Val Loss: 1.2517091035842896\n",
      "Epoch: 47 Step: 18125 Val Loss: 1.2505742311477661\n",
      "Epoch: 47 Step: 18150 Val Loss: 1.2537472248077393\n",
      "Epoch: 47 Step: 18175 Val Loss: 1.2529571056365967\n",
      "Epoch: 47 Step: 18200 Val Loss: 1.2477480173110962\n",
      "Epoch: 47 Step: 18225 Val Loss: 1.2518466711044312\n",
      "Epoch: 47 Step: 18250 Val Loss: 1.2544821500778198\n",
      "Epoch: 47 Step: 18275 Val Loss: 1.248720407485962\n",
      "Epoch: 47 Step: 18300 Val Loss: 1.255550742149353\n",
      "Epoch: 47 Step: 18325 Val Loss: 1.251766324043274\n",
      "Epoch: 48 Step: 18350 Val Loss: 1.253786325454712\n",
      "Epoch: 48 Step: 18375 Val Loss: 1.25706148147583\n",
      "Epoch: 48 Step: 18400 Val Loss: 1.2518845796585083\n",
      "Epoch: 48 Step: 18425 Val Loss: 1.253024697303772\n",
      "Epoch: 48 Step: 18450 Val Loss: 1.2588862180709839\n",
      "Epoch: 48 Step: 18475 Val Loss: 1.2502224445343018\n",
      "Epoch: 48 Step: 18500 Val Loss: 1.2499366998672485\n",
      "Epoch: 48 Step: 18525 Val Loss: 1.2502126693725586\n",
      "Epoch: 48 Step: 18550 Val Loss: 1.2533100843429565\n",
      "Epoch: 48 Step: 18575 Val Loss: 1.250023603439331\n",
      "Epoch: 48 Step: 18600 Val Loss: 1.2491674423217773\n",
      "Epoch: 48 Step: 18625 Val Loss: 1.2484713792800903\n",
      "Epoch: 48 Step: 18650 Val Loss: 1.2474392652511597\n",
      "Epoch: 48 Step: 18675 Val Loss: 1.2476813793182373\n",
      "Epoch: 48 Step: 18700 Val Loss: 1.247032642364502\n",
      "Epoch: 49 Step: 18725 Val Loss: 1.2525928020477295\n",
      "Epoch: 49 Step: 18750 Val Loss: 1.2501071691513062\n",
      "Epoch: 49 Step: 18775 Val Loss: 1.2530065774917603\n",
      "Epoch: 49 Step: 18800 Val Loss: 1.2519068717956543\n",
      "Epoch: 49 Step: 18825 Val Loss: 1.25021493434906\n",
      "Epoch: 49 Step: 18850 Val Loss: 1.25175142288208\n",
      "Epoch: 49 Step: 18875 Val Loss: 1.2485203742980957\n",
      "Epoch: 49 Step: 18900 Val Loss: 1.2554384469985962\n",
      "Epoch: 49 Step: 18925 Val Loss: 1.2503674030303955\n",
      "Epoch: 49 Step: 18950 Val Loss: 1.2506864070892334\n",
      "Epoch: 49 Step: 18975 Val Loss: 1.2537070512771606\n",
      "Epoch: 49 Step: 19000 Val Loss: 1.2565065622329712\n",
      "Epoch: 49 Step: 19025 Val Loss: 1.2560906410217285\n",
      "Epoch: 49 Step: 19050 Val Loss: 1.250287652015686\n",
      "Epoch: 49 Step: 19075 Val Loss: 1.2542788982391357\n",
      "Epoch: 49 Step: 19100 Val Loss: 1.2516192197799683\n"
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "  hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "  for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "      \n",
    "    tracker += 1\n",
    "    \n",
    "    x = one_hot_encoder(x,num_char)\n",
    "    inputs = torch.from_numpy(x)\n",
    "    targets = torch.from_numpy(y)\n",
    "    if model.use_gpu:\n",
    "        \n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "    # Reset Hidden State\n",
    "    # If we dont' reset we would backpropagate through all training history\n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    lstm_output, hidden = model.forward(inputs,hidden)\n",
    "    loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "    # LET\"S CLIP JUST IN CASE\n",
    "    nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    ### CHECK ON VALIDATION SET ######\n",
    "    \n",
    "    if tracker % 25 == 0:\n",
    "       \n",
    "      val_hidden = model.hidden_state(batch_size)\n",
    "      val_losses = []\n",
    "      model.eval()\n",
    "      \n",
    "      for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "  \n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "\n",
    "        if model.use_gpu:\n",
    "\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        val_hidden = tuple([state.data for state in val_hidden])\n",
    "        \n",
    "        lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "        val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "      \n",
    "      # Reset to training model after val for loop\n",
    "      model.train()\n",
    "      \n",
    "      print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBTT-LlQ5J0S"
   },
   "source": [
    "## 9) Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9G8MjUOOoefH"
   },
   "outputs": [],
   "source": [
    "model_name = 'trained_model.net'\n",
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2_IdNhR5aO7"
   },
   "source": [
    "## 10) Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4gE9jfm5VLj"
   },
   "outputs": [],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=character_set,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "-lMimGsM5sNb",
    "outputId": "99a400b7-e1c2-469b-c0cd-01c1f8c89991"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1-yqiZz52rP"
   },
   "source": [
    "## 11) Generating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRZlofwM5zna"
   },
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "  # Encode raw letters with model\n",
    "  encoded_text = model.encoder[char]\n",
    "  \n",
    "  # set as numpy array for one hot encoding\n",
    "  # NOTE THE [[ ]] dimensions!!\n",
    "  encoded_text = np.array([[encoded_text]])\n",
    "  \n",
    "  # One hot encoding\n",
    "  encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "  \n",
    "  # Convert to Tensor\n",
    "  inputs = torch.from_numpy(encoded_text)\n",
    "  \n",
    "  # Check for CPU\n",
    "  if(model.use_gpu):\n",
    "    inputs = inputs.cuda()\n",
    "  \n",
    "  \n",
    "  # Grab hidden states\n",
    "  hidden = tuple([state.data for state in hidden])\n",
    "  \n",
    "  \n",
    "  # Run model and get predicted output\n",
    "  lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "  \n",
    "  # Convert lstm_out to probabilities\n",
    "  probs = F.softmax(lstm_out, dim=1).data\n",
    "   \n",
    "  if(model.use_gpu):\n",
    "    # move back to CPU to use with numpy\n",
    "    probs = probs.cpu()\n",
    "\n",
    "  \n",
    "  # k determines how many characters to consider\n",
    "  # for our probability choice.\n",
    "  # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "  \n",
    "  # Return k largest probabilities in tensor\n",
    "  probs, index_positions = probs.topk(k)\n",
    "  \n",
    "  \n",
    "  index_positions = index_positions.numpy().squeeze()\n",
    "  \n",
    "  # Create array of probabilities\n",
    "  probs = probs.numpy().flatten()\n",
    "  \n",
    "  # Convert to probabilities per index\n",
    "  probs = probs/probs.sum()\n",
    "  \n",
    "  # randomly choose a character based on probabilities\n",
    "  char = np.random.choice(index_positions, p=probs)\n",
    "  \n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9aoM0Rs8cas"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "      \n",
    "  # CHECK FOR GPU\n",
    "  if(model.use_gpu):\n",
    "    model.cuda()\n",
    "  else:\n",
    "    model.cpu()\n",
    "  \n",
    "  # Evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  # begin output from initial seed\n",
    "  output_chars = [c for c in seed]\n",
    "  \n",
    "  # intiate hidden state\n",
    "  hidden = model.hidden_state(1)\n",
    "  \n",
    "  # predict the next character for every character in seed\n",
    "  for char in seed:\n",
    "    char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "  \n",
    "  # add initial characters to output\n",
    "  output_chars.append(char)\n",
    "  \n",
    "  # Now generate for size requested\n",
    "  for i in range(size):\n",
    "          \n",
    "    # predict based off very last letter in output_chars\n",
    "    char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "    \n",
    "    # add predicted character\n",
    "    output_chars.append(char)\n",
    "  \n",
    "  # return string of predicted text\n",
    "  return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "z1ji7tus82jS",
    "outputId": "ee4bd588-78e3-4d41-efff-8e394b6077cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fools and shallows,\n",
      "    And show'd the cheers to the true cheek to show,\n",
      "    Which will not had this trencher as to be\n",
      "    The summer's sons, as this that stirs the way.\n",
      "    This is not there, and what hath have been so.\n",
      "    The windows are not well and then a storm,\n",
      "    Which she shall be such as a state of him\n",
      "    To seek her fortune, which was that they stay\n",
      "    The brow to this aspect of that shall seem\n",
      "    To see her bearing shows, and there in time\n",
      "    We would not strike the world it. If you will,\n",
      "    The strength the state is there.\n",
      "  CORIOLANUS. The sea, think it in me.\n",
      "    I will not be a mortal to his honour\n",
      "    And set our country with her. Therefore, sir,\n",
      "    The season of their standings are as love\n",
      "    With their bestrews and statutes, and the throne\n",
      "    And banish'd them. When I had stand to hear\n",
      "    This the sound of their senses. Then there were they are\n",
      "    That this day shall be so much. This is me.\n",
      "    Thou hast been so true and a morning heart,\n",
      "    That weak a show of mine, and to be sounded,\n",
      "    And the traitor would not be.\n",
      "  ANTONY. To say thou art not, sir.\n",
      "  CLEOPATRA. If the state stands to tell me here is this.\n",
      "    Where should thou be? I have beguiled too soon.\n",
      "    This will I stay against your son, and to my\n",
      "    Warwick and your face. If your bed should be so\n",
      "    To be the matter of the world and me.\n",
      "    I have been too more than a soldier's state\n",
      "    To stay to this, I shall not straight and think\n",
      "    I will not be as sorry with my house.\n",
      "    We\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1500, seed='The ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhl_5tgq9C6_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
