# -*- coding: utf-8 -*-
"""autonomous_writer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ORDiq0_1gAzcIZvTddJJZXTltH0y_fW

#Predicting a sequence of words using NLP

## 1) Importing the basic libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

"""## 2) Reading the text"""

with open('./Data/shakespeare.txt') as f:
  text = f.read()

len(text), type(text)

text[:100]

print(text[:500])

"""## 3) Encoding and Decoding"""

# We are going for character level encoding

character_set = set(text)
print(f'{len(character_set)} \n')
print(np.array(character_set))

#Creating encoder and decoder

#Decoder (num --> letter)

decoder = dict(enumerate(character_set))

#Encoder (letter --> num)

encoder = {each:idx for idx,each in decoder.items()}

#Converting the text into numerical values

encoded_text = np.array([encoder[ch] for ch in text])
encoded_text[:500]

encoded_text.flatten()

"""To convert a numpy array into one hot encoding : https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array

### 3.1) Creating one-hot encoder
"""

#Creating one-hot encoder for the encoded_text

def one_hot_encoder(encoded_text,num_uni_chars):
  #encoded text ---> Batch of encoded text
  #num_uni_chars ---> len(set(text))

  one_hot = np.zeros((encoded_text.size,num_uni_chars))

  one_hot = one_hot.astype(np.float32)  #for tensor

  one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0

  one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))

  return one_hot

a = np.array([1,2,0])
print(a)
one_hot_encoder(a,3)

"""## 4) Generating Training Batches"""

def generate_batches(encoded_text,samp_per_batch=10,seq_length=50):
  
  # X  = encoded text of len sequence length(x to x+50)  
  # y = encoded text of len sequence length(x+1 to x+51)
  # For example : if encoded_text has [0,1,2,3,4,5,6] and seq_len = 3, then X = [0,1,2] , y = [1,2,3]

  #How many characters per batch ??
  char_per_batch = samp_per_batch * seq_length

  # How many batches we can make, given the length of encoding text ??
  num_batches_avail = int(len(encoded_text)/char_per_batch)

  #Cut off the borders of the encoded text that does not fit in this range
  encoded_text = encoded_text[:char_per_batch * num_batches_avail]

  encoded_text = encoded_text.reshape((samp_per_batch,-1))

  # Go through each row in array.
  for n in range(0, encoded_text.shape[1], seq_length):
      
    # Grab feature characters
    x = encoded_text[:, n:n+seq_length]
    
    # y is the target shifted over by 1
    y = np.zeros_like(x)
    
    #
    try:
        y[:, :-1] = x[:, 1:]
        y[:, -1]  = encoded_text[:, n+seq_length]
        
    # FOR POTENTIAL INDEXING ERROR AT THE END    
    except:
        y[:, :-1] = x[:, 1:]
        y[:, -1] = encoded_text[:, 0]
        
    yield x, y

sample_text = np.arange(20)
sample_text

sam = generate_batches(sample_text,2,10)
x,y = next(sam)
print(x)
print(y)

"""## 5) Creating the LSTM model"""

class CharModel(nn.Module):
    
  def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):
      
      
      # SET UP ATTRIBUTES
      super().__init__()
      self.drop_prob = drop_prob
      self.num_layers = num_layers
      self.num_hidden = num_hidden
      self.use_gpu = use_gpu
      
      #CHARACTER SET, ENCODER, and DECODER
      self.all_chars = all_chars
      self.decoder = dict(enumerate(all_chars))
      self.encoder = {char: ind for ind,char in decoder.items()}
      
      
      self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)
      
      self.dropout = nn.Dropout(drop_prob)
      
      self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))
    
  
  def forward(self, x, hidden):
                
      
      lstm_output, hidden = self.lstm(x, hidden)
      
      
      drop_output = self.dropout(lstm_output)
      
      drop_output = drop_output.contiguous().view(-1, self.num_hidden)
      
      
      final_out = self.fc_linear(drop_output)
      
      
      return final_out, hidden
  
  
  def hidden_state(self, batch_size):
      
      if self.use_gpu:
          
          hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),
                    torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())
      else:
          hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),
                    torch.zeros(self.num_layers,batch_size,self.num_hidden))
      
      return hidden

model = CharModel(
    all_chars=character_set,
    num_hidden=512,
    num_layers=3,
    drop_prob=0.5,
    use_gpu=True,
)

total_param  = []
for p in model.parameters():
    total_param.append(int(p.numel()))

sum(total_param)

"""## 6) Optimization and Loss Function"""

optimizer = torch.optim.Adam(model.parameters(),lr=0.001)
criterion = nn.CrossEntropyLoss()

"""## 7) Training Data and Validation Data"""

train_percent = 0.9
int(len(encoded_text)*train_percent)

train_ind = int(len(encoded_text)*train_percent)
train_data = encoded_text[:train_ind]
val_data = encoded_text[train_ind:]

"""## 8) Training the network"""

## VARIABLES

# Epochs to train for
epochs = 50
# batch size 
batch_size = 128

# Length of sequence
seq_len = 100

# for printing report purposes
# always start at 0
tracker = 0

# number of characters in text
num_char = max(encoded_text)+1

# Set model to train
model.train()


# Check to see if using GPU
if model.use_gpu:
    model.cuda()

for i in range(epochs):
    
  hidden = model.hidden_state(batch_size)
    
  for x,y in generate_batches(train_data,batch_size,seq_len):
      
    tracker += 1
    
    x = one_hot_encoder(x,num_char)
    inputs = torch.from_numpy(x)
    targets = torch.from_numpy(y)
    if model.use_gpu:
        
        inputs = inputs.cuda()
        targets = targets.cuda()
        
    # Reset Hidden State
    # If we dont' reset we would backpropagate through all training history
    hidden = tuple([state.data for state in hidden])
    
    model.zero_grad()
    
    lstm_output, hidden = model.forward(inputs,hidden)
    loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())
    
    loss.backward()
    
    # POSSIBLE EXPLODING GRADIENT PROBLEM!
    # LET"S CLIP JUST IN CASE
    nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)
    
    optimizer.step()
    
    ### CHECK ON VALIDATION SET ######
    
    if tracker % 25 == 0:
       
      val_hidden = model.hidden_state(batch_size)
      val_losses = []
      model.eval()
      
      for x,y in generate_batches(val_data,batch_size,seq_len):
  
        x = one_hot_encoder(x,num_char)
        inputs = torch.from_numpy(x)
        targets = torch.from_numpy(y)

        if model.use_gpu:

            inputs = inputs.cuda()
            targets = targets.cuda()
            
        val_hidden = tuple([state.data for state in val_hidden])
        
        lstm_output, val_hidden = model.forward(inputs,val_hidden)
        val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())

        val_losses.append(val_loss.item())
      
      # Reset to training model after val for loop
      model.train()
      
      print(f"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}")

"""## 9) Saving the model"""

model_name = 'trained_model.net'
torch.save(model.state_dict(),model_name)

"""## 10) Loading the model"""

# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!

model = CharModel(
    all_chars=character_set,
    num_hidden=512,
    num_layers=3,
    drop_prob=0.5,
    use_gpu=True,
)

model.load_state_dict(torch.load(model_name))
model.eval()

"""## 11) Generating the predictions"""

def predict_next_char(model, char, hidden=None, k=1):
        
  # Encode raw letters with model
  encoded_text = model.encoder[char]
  
  # set as numpy array for one hot encoding
  # NOTE THE [[ ]] dimensions!!
  encoded_text = np.array([[encoded_text]])
  
  # One hot encoding
  encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))
  
  # Convert to Tensor
  inputs = torch.from_numpy(encoded_text)
  
  # Check for CPU
  if(model.use_gpu):
    inputs = inputs.cuda()
  
  
  # Grab hidden states
  hidden = tuple([state.data for state in hidden])
  
  
  # Run model and get predicted output
  lstm_out, hidden = model(inputs, hidden)

  
  # Convert lstm_out to probabilities
  probs = F.softmax(lstm_out, dim=1).data
   
  if(model.use_gpu):
    # move back to CPU to use with numpy
    probs = probs.cpu()

  
  # k determines how many characters to consider
  # for our probability choice.
  # https://pytorch.org/docs/stable/torch.html#torch.topk
  
  # Return k largest probabilities in tensor
  probs, index_positions = probs.topk(k)
  
  
  index_positions = index_positions.numpy().squeeze()
  
  # Create array of probabilities
  probs = probs.numpy().flatten()
  
  # Convert to probabilities per index
  probs = probs/probs.sum()
  
  # randomly choose a character based on probabilities
  char = np.random.choice(index_positions, p=probs)
  
  # return the encoded value of the predicted char and the hidden state
  return model.decoder[char], hidden

def generate_text(model, size, seed='The', k=1):
      
  # CHECK FOR GPU
  if(model.use_gpu):
    model.cuda()
  else:
    model.cpu()
  
  # Evaluation mode
  model.eval()
  
  # begin output from initial seed
  output_chars = [c for c in seed]
  
  # intiate hidden state
  hidden = model.hidden_state(1)
  
  # predict the next character for every character in seed
  for char in seed:
    char, hidden = predict_next_char(model, char, hidden, k=k)
  
  # add initial characters to output
  output_chars.append(char)
  
  # Now generate for size requested
  for i in range(size):
          
    # predict based off very last letter in output_chars
    char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)
    
    # add predicted character
    output_chars.append(char)
  
  # return string of predicted text
  return ''.join(output_chars)

print(generate_text(model, 1500, seed='The ', k=3))

